<!DOCTYPE html>
<html data-wf-domain="" data-wf-page="596e65d120426e09785027f0" data-wf-site="596e65d120426e09785027eb" data-wf-status="1"
    class="w-mod-js wf-opensans-n3-active wf-opensans-n4-active wf-roboto-n4-active wf-opensans-i3-active wf-opensans-i4-active wf-opensans-n6-active wf-opensans-i6-active wf-opensans-n7-active wf-opensans-i7-active wf-opensans-n8-active wf-opensans-i8-active wf-roboto-n3-active wf-roboto-n5-active wf-active">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>T2Bs</title>
    <meta content="width=device-width, initial-scale=1" name="viewport">
    <meta content="Webflow" name="generator">
    <link href="./files/supplemental.css" rel="stylesheet" type="text/css">
    <script src="./files/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
</head>


<body class="body">
    <div class="section">
        <div class="container-3 w-container">
            <h1 class="papertitle">T2Bs: Text-to-Character Blendshapes via Video Generation</h1>
            <div class="text-block">Submission ID: 7066</div>
        </div>
    </div>

    <div class="section-2">
        <div class="container w-container">
            <ul role="list" class="list">

                <a href="#experiment_1">1. T2Bs models</a>
            </li>
            <ul role="list">
                <li>
                    <a href="#experiment_1_1">1.1 Blendshape Gallery</a>
                </li>
                <li>
                    <a href="#experiment_1_2">1.2 Web Demo</a>
                </li>
                <li>
                    <a href="#experiment_1_3">1.3 Model Expressiveness</a>
                </li>
                <li>
                    <a href="#experiment_1_4">1.4 Retargeting</a>
                </li>
            </ul>

                    <a href="#experiment_2_1">2. View-conditioned Deformable Gaussian Splatting(VCDGS)</a>
                    </li>
                    <ul role="list">
                        <li>
                            <a href="#experiment_2_1">2.1 Comparison with baseline methods</a>
                        </li>
                        <li>
                            <a href="#experiment_2_2">2.2 Ablation studies</a>
                        </li>
                        <!-- <li>
                            <a href="#experiment_1_4">2.3 Model fitting</a>
                        </li> -->
                    </ul>
                
            </ul>
        </div>
    </div>

    <div>
        <div class="container-2 w-container">
            <div class="container-2 w-container">
                <h3 id="experiment_1_1" class="experimenttitle">1. T2Bs models</h3>
            </div>
            <div class="w-container">
                <h3 id="experiment_1_1" class="subexperimenttitle">1.1 Blendshape Gallery</h3>
                <p class="paragraph">
                    Text-to-character blendshapes (T2Bs) is capable of creating animatable blendshapes to synthesize diverse expressions of a virtual character generated solely from text prompts. We show the geometry of sample eigen expressions of 4 characters in the following videos.<br><br>
                </p>

                <div class="videoresult w-row" style="display: flex; flex-wrap: nowrap; justify-content: space-between;">
                    <div class="w-col" style="flex: 1; padding: 5px;">
                        <center><span style="font-size: 12pt; color: green;"><b>Dog</b></span></center>
                        <video width="100%" height="100%" src="./files/video/gallary/dog.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    </div>
                </div>

                <div class="videoresult w-row" style="display: flex; flex-wrap: nowrap; justify-content: space-between;">
                    <div class="w-col" style="flex: 1; padding: 5px;">
                        <center><span style="font-size: 12pt; color: green;"><b>Frog</b></span></center>
                        <video width="100%" height="100%" src="./files/video/gallary/frog.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    </div>
                </div>

                <div class="videoresult w-row" style="display: flex; flex-wrap: nowrap; justify-content: space-between;">
                    <div class="w-col" style="flex: 1; padding: 5px;">
                        <center><span style="font-size: 12pt; color: green;"><b>Donkey</b></span></center>
                        <video width="100%" height="100%" src="./files/video/gallary/donkey.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    </div>
                </div>

                <div class="videoresult w-row" style="display: flex; flex-wrap: nowrap; justify-content: space-between;">
                    <div class="w-col" style="flex: 1; padding: 5px;">
                        <center><span style="font-size: 12pt; color: green;"><b>Fox</b></span></center>
                        <video width="100%" height="100%" src="./files/video/gallary/fox.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    </div>
                </div>

                <hr>


                <h3 id="experiment_1_2" class="subexperimenttitle">1.2 Web Demo</h3>

                <p class="paragraph">
                    We demonstrate demos of the first 10 eigen expressions of sample virtual character models as vidualized in the following video. Note that for later model evaluation and retargeting we use 100 eigen expressions.<br><br>
                </p>

                <video width="100%" height="100%" src="./files/video/demo_fox.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                <video width="100%" height="100%" src="./files/video/demo_dog.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>


            <br><br><br><br>

                <h3 id="experiment_1_3" class="subexperimenttitle">1.3 Model Expressiveness</h3>
                <p class="paragraph">
                    A robust statistical expression model should effectively generalize to new data while remaining closely aligned with the specific object it represents. We fit the T2Bs model to captures outside the model’s training set. We compared the rendered Gaussians and the mesh geometry and calculate the error shown in the following. For each identity, the first column is model fitting, the second column is a held-out video capture, and the third column are image-space and object-space reconstruction error map. The color scale from blue to yellow represents the RGB error, ranging from 0 to 0.5, while the scale from green to red denotes the 3D point-to-point error, ranging from 0 to 1/500 of the bounding box size. The bounding box size is approximately the maximum possible distance within the geometry. The learned blendshape can faithfully reconstruct meshes with held-out expressions.<br><br>
                </p>
                <div class="w-col">
                    <center><span style="font-size: 12pt; color: green;"><b>Model fitting (Fig. 10)</b></span></center>
                    <video width="100%" height="100%" src="./files/video/fig10.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                </div>

                <hr>


                <h3 id="experiment_1_4" class="subexperimenttitle">1.4 Retargeting</h3>
                <p class="paragraph">
                    We further show the expressiveness of our model by adding a simple landmark-base retargeting approach, in which we align the human (FLAME) landmarks with 20 anotated animal landmarks on eyes and mouth regions. We show both the retargeting results with the eigen expressions of a human model (FLAME), and with the real facial capture from videos.<br><br>
                </p>
                <div class="w-col">
                    <center><span style="font-size: 12pt; color: green;"><b>Model retargeting (FLAME eigen expressions)</b></span></center>
                    <video width="100%" height="100%" src="./files/video/fig6_.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    <center><span style="font-size: 12pt; color: green;"><b>Model retargeting (real capture)</b></span></center>
                    <video width="100%" height="100%" src="./files/video/fig6_du.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    <video width="100%" height="100%" src="./files/video/fig6_tr.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                </div>

                <hr>


            <br><br><br><br>

            <div class="w-container">
                <h3 id="experiment_2_1" class="subexperimenttitle">2. View-conditioned Deformable Gaussian Splatting (VCDGS)</h3>
                <br>
                <h3 id="experiment_2_1" class="subexperimenttitle">2.1 Comparison with baseline methods in novel view synthesis</h3>
                <p class="paragraph">
                    Qualitative comparison of 4D generation methods. We compare the 4D generation results of our method with DreamGaussian4D (DG4D) [38], SV4D [64], and 4Real-Video [53]. All methods take a monocular video as input. In addition, DG4D incorporates our accurate static Gaussian representation, whereas SV4D and 4Real-Video rely on freeze-time renderings. Our method takes the results of 4Real-Video as input and further enhances the novel view synthesis. Notably, none of the baseline methods produce high-quality 3D geometry, whereas ours does. Viewpoints are displayed at ±60 degrees relative to the original perspective (frontal view) used for generating the monocular video. We also show the rendering of the static mesh from the same viewpoint as 'static reference'. Among all methods, our approach achieves the most visually consistent and appealing results.<br><br>
                </p>
                <br>
                <div class="videoresult w-row">
                    <div class="w-col">
                        <div style="display: flex; justify-content: space-between; text-align: center; font-size: 12pt; color: green; font-weight: bold; margin: 0 2%; ">
                            <span>Monocular Input</span>
                            <span>DG4D</span>
                            <span>SV4D</span>
                            <span>4Real-Video</span>
                            <span>Ours</span>
                            <span>Static Reference</span>
                        </div>
                        <video width="100%" height="100%" src="./files/video/baseline/moose.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/lion.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/cat_1.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/bear.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/bunny.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/eagle.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/cat.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/frog.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/unicorn_2.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                        <video width="100%" height="100%" src="./files/video/baseline/pigly.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    </div>
                </div>

                <br>

<!--                 <p class="paragraph">
                    <strong>We show more baseline comparison video <a href="more_baseline_comparison.html">here</a> with more than 200 examples.</strong><br><br>
                </p> -->




                <hr>
                
            <br><br><br><br>
                



    <div class="w-container">
                <h3 id="experiment_2_2" class="subexperimenttitle">2.2 Ablation Studies</h3>

                <h3 class="subexperimenttitle" style="font-size: 1.25em;">2.2.1 Ablation study on view dependency</h3>
                <p class="paragraph">
                    A qualitative comparison between VCDGS with and without camera view information as input. The model without view dependency produces noticeable artifacts, as it struggles to regress into a coherent geometry from 3D inconsistencies across views in multiview videos during the 4D generation process.<br><br>
                </p>

                <h3 class="subexperimenttitle" style="font-size: 1.25em;">2.2.2 Ablation study on source of multi-view video</h3>
                <p class="paragraph">
                    We optimize VCDGS using multiple videos generated by 4Real-video. To evaluate its effectiveness, we conduct an ablation study using relatively lower-quality 4D videos as guidance. Specifically, we train VCDGS on the output of SV4D while keeping all other experimental settings unchanged. Figure~\ref{fig:sv4d} demonstrates the improvements achieved by incorporating VCDGS into SV4D. In our application, SV4D tends to produce blurry novel views. Our model not only remains robust to blurry inputs but also generates high-quality renderings with well-defined geometry.<br><br>
                </p>
                
                <br>
                <div style="display: flex; gap: 10px;">
                    <!-- 1/3 width video -->
                    <div style="flex: 1;">
                        <center><span style="font-size: 12pt; color: green;"><b>Ablation study on view dependency (Fig. 6)</b></span></center>
                        <br>
                        <div style="display: flex; text-align: center; font-size: 12pt; color: black; font-weight: bold; margin: 0 2%;">
                            <span style="flex: 1; text-align: center;">Input</span>
                            <span style="flex: 1; text-align: center;">Ours</span>
                            <span style="flex: 1; text-align: center;">w/o view dependency</span>
                        </div>
                        <video width="100%" height="100%" src="./files/video/fig6.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    </div>

                    <div style="flex: 1;">
                        <center><span style="font-size: 12pt; color: green;"><b>Ablation study on source of multi-view video (Fig. 7)</b></span></center>
                        <br>
                        <div style="display: flex; justify-content: space-between; text-align: center; font-size: 12pt; color: black; font-weight: bold; margin: 0 7%; ">
                            <span style="flex: 1; text-align: center;">Input</span>
                            <span style="flex: 1; text-align: center;">SV4D</span>
                            <span style="flex: 1; text-align: center;">SV4D + VCDGS</span>
                        </div>
                        <video width="100%" height="100%" src="./files/video/fig7.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                    </div>

                </div>

                <br><br><br><br>


                <h3 class="subexperimenttitle" style="font-size: 1.25em;">2.2.3 Ablation study on number of control points</h3>
                <p class="paragraph">
                    We use pre-define control points from static asset rather than jointly optimizing them with all expression videos for scalability. This allowing new expression videos to be incorporated without requiring re-optimization of previously processed videos. This modular approach avoids the need for computationally expensive joint optimization across an ever-growing dataset. We use 2000 control points by uniformly sample the static mesh. We show parameter analysis on the the number of control points in figure X.  2000 control points captures fine-grained motions, such as tongue (1st row) and eyelid (2nd row) movements, better than 200, 500, or 1000 joints.<br><br>
                </p>

                <div style="flex: 2;">
                    <center><span style="font-size: 12pt; color: green;"><b>Ablation study on number of control points</b></span></center>
                    <br>
                    <div style="display: flex; justify-content: space-between; text-align: center; font-size: 12pt; color: black; font-weight: bold; margin: 0 7%; ">
                        <span>Input</span>
                        <span>200</span>
                        <span>500</span>
                        <span>1000</span>
                        <span>2000</span>
                    </div>
                    <video width="100%" height="100%" src="./files/video/figx.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                </div>

                <br><br>

                <hr>

                <br><br><br><br>


    
                <!-- <p id="experiment_1_4" class="paragraph">
                    2.3 Morphable model fitting to the test set <br><br>
                </p>
                <div class="w-col">
                    <center><span style="font-size: 12pt; color: green;"><b>Model fitting (Fig. 10)</b></span></center>
                    <video width="100%" height="100%" src="./files/video/fig10.mp4" type="video/mp4" loop="true" autoplay="autoplay" controls muted></video>
                </div>

                <hr> -->