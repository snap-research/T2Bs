<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="T2Bs: Text-to-Character Blendshapes via Video Generation">
  <meta name="keywords" content="blendshape, animal model, animal face">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title></title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">T2Bs: Text-to-Character Blendshapes via Video Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <strong>ICCV 2025</strong>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jiahao Luo, Chaoyang Wang, Michael Vasilkovsky, Vladislav Shakhrai, Di Liu, Peiye Zhuang, Sergey Tulyakov, Peter Wonka, Hsin-Ying Lee, James Davis, Jian Wang
            </span>
          </div>

          <div class="is-size-5 Institude">
            <span class="author-block">
              Snap Inc., University of California, Santa Cruz
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <span class="link-block">
                <a href="https://arxiv.org/pdf/2509.10678"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>   <!-- PDF icon -->
                  </span>
                  <span>PDF</span>
                </a>
              </span>

                            <span class="link-block">
                <a href="./supp.html"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>   <!-- PDF icon -->
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>

              
              <!-- Code Link. -->
<!--               <span class="link-block">
                <a href="https://github.com/---------(Anonymous)"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset (coming soon!)</span>
                  </a>
              </span> -->

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/teaser.jpg"/>

      <div class="columns is-multiline">
        <div class="column is-half has-text-centered">
          <video class="mt-2" autoplay loop muted controls width="100%">
            <source src="./files/video/demo_fox.mp4" type="video/mp4">
          </video>
        </div>

        <div class="column is-half has-text-centered">
          <video class="mt-2" autoplay loop muted controls width="100%">
            <source src="./files/video/demo_dog.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="has-text-centered">
        <video class="mt-2 mb-3" autoplay loop muted controls width="100%">
          <source src="./files/video/retargeting.mp4" type="video/mp4">
        </video>
      </div>

      <p>
        Text-to-character blendshapes (T2Bs) is capable of creating animatable blendshapes to synthesize diverse expressions of a virtual character generated solely from text prompts.
      </p>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We present T2Bs, a framework for generating high-quality, animatable character head morphable models from text by combining static text-to-3D generation with video diffusion. Text-to-3D models produce detailed static geometry but lack motion synthesis, while video diffusion models generate motion with temporal and multi-view geometric inconsistencies. T2Bs bridges this gap by leveraging deformable 3D Gaussian splatting to align static 3D assets with video outputs. By constraining motion with static geometry and employing a view-dependent deformation MLP, T2Bs (i) outperforms existing 4D generation methods in accuracy and expressiveness while reducing video artifacts and view inconsistencies, and (ii) reconstructs smooth, coherent, fully registered 3D geometries designed to scale for building morphable models with diverse, realistic facial motions. This enables synthesizing expressive, animatable character heads that surpass current 4D generation techniques.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

  

<section class="hero teaser">
  <div class="container is-max-desktop">
    <h2 class="title is-3" style="text-align: center;">Method</h2>
    <div class="hero-body">
        <img src="./static/images/pipeline.png"/>
      <p><b>In the first part, we illustrate the generation of multi-view videos from text prompts. A static 3D mesh is first created using an off-the-shelf text-to-3D generator~\cite{trellis3d}, followed by rendering a fixed-time video with the camera moving in a circular path. We define a canonical view (v=0) and an augmented prompt to generate a fixed-view video. A 4D video generation method is then applied to produce multi-view videos. In the second part, starting from a static 3D asset, we define static Gaussians $G_{0, 0}$, control points $p$ and blending weights $w$. During deformation, we predict view-dependent transformations of control points to model local non-rigid deformations, along with global transformations to capture overall pose changes. We interpolate Gaussian positions and orientations with Linear Blend Skinning (LBS), with rendering optimized through image-space loss minimization. After training, we extract a mesh for each frame, defined in the canonical view  (v=0). We repeat this process with multiple prompts, and build a blendshape model using hundreds of samples</p>
    </div>
  </div>
</section>




<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Demo Video</h2>
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/KinMo-Demo.mp4"
            type="video/mp4">
          </video>
        </div>
    </div>
  </div>
</section> -->





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code>@article{luo2025t2bs,
  title={T2Bs: Text-to-Character Blendshapes via Video Generation},
  author={Luo, Jiahao and Wang, Chaoyang and Vasilkovsky, Michael and Shakhrai, Vladislav and Liu, Di and Zhuang, Peiye and Tulyakov, Sergey and Wonka, Peter and Lee, Hsin-Ying and Davis, James and others},
  journal={arXiv preprint arXiv:2509.10678},
  year={2025}
}</code></pre>
  </div>
</section>

<footer class="footer" style="padding-top: 6px; padding-bottom: 6px;">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Our website template is a modified version of <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. Thanks to the authors' contribution.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
